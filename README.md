# Agentic Document Intelligence

## Project Overview
The Intelligent Legal Assistant is an advanced AI-driven system designed to automate the extraction, analysis, and reasoning of complex legal contracts. Unlike traditional LegalTech tools that rely on fragile OCR and keyword matching, this project leverages a multi-model architecture combining Vision-Language Models (VLM) for structural understanding and Small Language Models (SLM) for high-precision legal reasoning.

The system is built to handle the specific nuances of legal documents, including multi-column layouts, embedded tables, checkboxes, and cross-referenced clauses. It aims to reduce the time legal professionals spend on manual review by providing an interactive Q&A interface.

## Architecture Overview
The system architecture is biologically inspired, composed of four distinct modules that function in unison to process information from visual input to cognitive reasoning.

1. The Eye: Vision-Language Model (VLM)
Role: Structural Extraction & Layout Analysis.

Function: This module acts as the ingestion layer. It takes raw PDF images (contract pages) and converts them into structured machine-readable text (Markdown/JSON). Crucially, it preserves visual context—tables remain tables, checkboxes are identified as checked/unchecked, and headers maintain their hierarchy. It replaces traditional OCR engines which often fail on complex layouts.

2. The Memory: Retrieval-Augmented Generation (RAG)
Role: Context Indexing & Retrieval.

Function: This module solves the "context window" limitation of LLMs. It indexes the structured text generated by the VLM using a hierarchy-aware strategy (Section -> Clause). It ensures that when a user asks a question, the system retrieves not just the relevant sentence, but the surrounding context necessary for interpretation.

3. The Brain: Specialized Small Language Model (SLM)
Role: Reasoning, Risk Assessment & Entity Extraction.

Function: This is the cognitive core. It takes the retrieved context from "The Memory" and performs specific legal tasks. It is fine-tuned to act as a rigorous legal auditor, capable of flagging risks, extracting specific entities (dates, parties, amounts), and answering natural language queries with citations.

4. The Face: User Interface (UI)
Role: Interaction & Visualization.

Function: The frontend layer (AI chatbot in Telegram) that displays the original PDF alongside the AI's analysis. It features interactive referencing, allowing users to click on an AI-generated answer and immediately see the source clause highlighted in the original document.

## Technical Implementation
### Part 1: Fine-tuning the VLM (The Eye)
The primary objective is to replace traditional OCR (Optical Character Recognition) systems. Standard OCR often produces "text soup"—streams of unformatted text that lose the spatial relationships defining a contract. Our fine-tuned VLM must look at a contract page and output clean, structured Markdown or JSON, effectively ignoring noise such as watermarks or scan artifacts while correctly interpreting visual signals like signatures and checkboxes.

### Part 2: RAG Pipeline Construction (The Memory)
The objective is to retrieve the exact legal clause relevant to a user's query while preserving the semantic context of the surrounding section. Standard RAG approaches often fail in legal contexts because they rely on simple token splitting, which can sever a defined term from its definition or a condition from its exception.

Chunking Strategy: "Parent-Child" Indexing: to solve the context fragmentation issue, we utilize a Parent-Child indexing strategy:

+ Parent Documents: The document is first segmented by logical sections (e.g., "Article 5: Termination", "Section 2: Confidentiality"). These large blocks are stored as the "Parent" chunks.

+ Child Chunks: The Parent documents are further broken down into smaller, dense chunks (e.g., 256 tokens). These are the "Child" chunks used for vector search.

+ Retrieval Logic: The system performs the vector search against the Child chunks to find the most specific matches. However, once a match is found, the system retrieves and feeds the Parent chunk to the LLM. This ensures the model sees the full clause and surrounding context, not just a sentence fragment.

Advanced Retrieval Methods
+ Hybrid Search: We combine BM25 (Keyword Search) with Vector Search (Semantic Search).

+ Re-ranking: A Cross-Encoder (specifically `bge-reranker-v2-m3`) is used to re-score the top 15 retrieved chunks.

+ Multi-query Expansion: The system generates variations of the user's query to cast a wider net during the initial search phase, increasing the recall of relevant clauses.

### Part 3: Fine-tuning the Specialized SLM (The Brain)
We need to engineer a multi-task reasoning engine. This model must process the context retrieved by the RAG pipeline to perform question answering with sub-second latency:


### Part 4: Telegram Chatbot UI (The Face)
