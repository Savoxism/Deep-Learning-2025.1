import torch
from sentence_transformers import SentenceTransformer
from typing import List, Union
import numpy as np

class EmbeddingModel:
    def __init__(self, model_name: str = "intfloat/multilingual-e5-base", max_length: int = 512, device: str = None):
        """
        Wrapper class cho E5 Embedding Model.
        Tá»± Ä‘á»™ng xá»­ lÃ½ prefix 'query:' vÃ  'passage:' theo yÃªu cáº§u cá»§a E5 paper.
        """
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ðŸ”§ Loading model {model_name} on {self.device}...")
        
        self.model = SentenceTransformer(model_name, device=self.device)
        self.model.max_seq_length = max_length
        self.max_length = max_length

    def encode_queries(self, queries: List[str], batch_size: int = 16) -> np.ndarray:
        """Encode cÃ¢u há»i (thÃªm prefix 'query: ')"""
        # E5 yÃªu cáº§u prefix 'query: ' cho cÃ¢u há»i
        processed_queries = [f"query: {q}" for q in queries]
        
        return self.model.encode(
            processed_queries,
            batch_size=batch_size,
            normalize_embeddings=True,
            show_progress_bar=False,
            convert_to_numpy=True
        )

    def encode_documents(self, documents: List[str], batch_size: int = 16) -> np.ndarray:
        """Encode vÄƒn báº£n/corpus (thÃªm prefix 'passage: ')"""
        # E5 yÃªu cáº§u prefix 'passage: ' cho documents
        processed_docs = [f"passage: {d}" for d in documents]
        
        return self.model.encode(
            processed_docs,
            batch_size=batch_size,
            normalize_embeddings=True,
            show_progress_bar=True,
            convert_to_numpy=True
        )